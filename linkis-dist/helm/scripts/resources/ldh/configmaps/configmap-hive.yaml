# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: hive-conf
data:
  hive-env.sh: |
    # Hive Client memory usage can be an issue if a large number of clients
    # are running at the same time. The flags below have been useful in
    # reducing memory usage:
    #
    # if [ "$SERVICE" = "cli" ]; then
    #   if [ -z "$DEBUG" ]; then
    #     export HADOOP_OPTS="$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseParNewGC -XX:-UseGCOverheadLimit"
    #   else
    #     export HADOOP_OPTS="$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit"
    #   fi
    # fi

    # The heap size of the jvm stared by hive shell script can be controlled via:
    #
    # export HADOOP_HEAPSIZE=1024
    #
    # Larger heap size may be required when running queries over large number of files or partitions.
    # By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be
    # appropriate for hive server.


    # Set HADOOP_HOME to point to a specific hadoop install directory
    HADOOP_HOME=/opt/ldh/current/hadoop

    # Hive Configuration Directory can be controlled by:
    export HIVE_CONF_DIR=/etc/ldh/hive

    # Folder containing extra libraries required for hive compilation/execution can be controlled by:
    # export HIVE_AUX_JARS_PATH=

  hive-site.xml: |
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
          <name>javax.jdo.option.ConnectionURL</name>
          <value>jdbc:mysql://mysql.mysql.svc.cluster.local:3306/hive_metadata?&amp;createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
      </property>
      <property>
          <name>javax.jdo.option.ConnectionUserName</name>
          <value>root</value>
      </property>
      <property>
          <name>javax.jdo.option.ConnectionPassword</name>
          <value>123456</value>
      </property>
      <property>
          <name>javax.jdo.option.ConnectionDriverName</name>
          <value>com.mysql.jdbc.Driver</value>
      </property>
      <property>
          <name>datanucleus.schema.autoCreateAll</name>
          <value>true</value>
      </property>
      <property>
          <name>hive.metastore.schema.verification</name>
          <value>false</value>
      </property>
      <property>
        <name>hive.metastore.uris</name>
        <value>thrift://ldh.ldh.svc.cluster.local:9083</value>
      </property>
    </configuration>

  beeline-log4j2.properties: |
    status = INFO
    name = BeelineLog4j2
    packages = org.apache.hadoop.hive.ql.log

    # list of properties
    property.hive.log.level = WARN
    property.hive.root.logger = console

    # list of all appenders
    appenders = console

    # console appender
    appender.console.type = Console
    appender.console.name = console
    appender.console.target = SYSTEM_ERR
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n

    # list of all loggers
    loggers = HiveConnection

    # HiveConnection logs useful info for dynamic service discovery
    logger.HiveConnection.name = org.apache.hive.jdbc.HiveConnection
    logger.HiveConnection.level = INFO

    # root logger
    rootLogger.level = ${sys:hive.log.level}
    rootLogger.appenderRefs = root
    rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}

  hive-exec-log4j2.properties: |
    status = INFO
    name = HiveExecLog4j2
    packages = org.apache.hadoop.hive.ql.log

    # list of properties
    property.hive.log.level = INFO
    property.hive.root.logger = FA
    property.hive.query.id = hadoop
    property.hive.log.dir = /var/log/hive/${sys:user.name}
    property.hive.log.file = ${sys:hive.query.id}.log

    # list of all appenders
    appenders = console, FA

    # console appender
    appender.console.type = Console
    appender.console.name = console
    appender.console.target = SYSTEM_ERR
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n

    # simple file appender
    appender.FA.type = RandomAccessFile
    appender.FA.name = FA
    appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
    appender.FA.layout.type = PatternLayout
    appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n

    # list of all loggers
    loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX

    logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
    logger.NIOServerCnxn.level = WARN

    logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
    logger.ClientCnxnSocketNIO.level = WARN

    logger.DataNucleus.name = DataNucleus
    logger.DataNucleus.level = ERROR

    logger.Datastore.name = Datastore
    logger.Datastore.level = ERROR

    logger.JPOX.name = JPOX
    logger.JPOX.level = ERROR

    # root logger
    rootLogger.level = ${sys:hive.log.level}
    rootLogger.appenderRefs = root
    rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}

  hive-log4j2.properties: |
    status = INFO
    name = HiveLog4j2
    packages = org.apache.hadoop.hive.ql.log

    # list of properties
    property.hive.log.level = INFO
    property.hive.root.logger = DRFA
    property.hive.log.dir = /var/log/hive/${sys:user.name}
    property.hive.log.file = hive.log
    property.hive.perflogger.log.level = INFO

    # list of all appenders
    appenders = console, DRFA

    # console appender
    appender.console.type = Console
    appender.console.name = console
    appender.console.target = SYSTEM_ERR
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n

    # daily rolling file appender
    appender.DRFA.type = RollingRandomAccessFile
    appender.DRFA.name = DRFA
    appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
    # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
    appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
    appender.DRFA.layout.type = PatternLayout
    appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
    appender.DRFA.policies.type = Policies
    appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
    appender.DRFA.policies.time.interval = 1
    appender.DRFA.policies.time.modulate = true
    appender.DRFA.strategy.type = DefaultRolloverStrategy
    appender.DRFA.strategy.max = 30

    # list of all loggers
    loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger

    logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
    logger.NIOServerCnxn.level = WARN

    logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
    logger.ClientCnxnSocketNIO.level = WARN

    logger.DataNucleus.name = DataNucleus
    logger.DataNucleus.level = ERROR

    logger.Datastore.name = Datastore
    logger.Datastore.level = ERROR

    logger.JPOX.name = JPOX
    logger.JPOX.level = ERROR

    logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
    logger.PerfLogger.level = ${sys:hive.perflogger.log.level}

    # root logger
    rootLogger.level = ${sys:hive.log.level}
    rootLogger.appenderRefs = root
    rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
